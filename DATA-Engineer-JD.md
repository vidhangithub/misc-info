Job Description â€“ GCP Data Engineer

Role Overview
We are seeking a highly skilled GCP Data Engineer to join our team and play a key role in building scalable, reusable, and high-performing data solutions. The ideal candidate will be responsible for designing and developing data pipelines, working with structured and unstructured data, enabling machine learning feature engineering, and delivering real-time contextual insights that power personalized customer journeys.

This role will involve working with industry-leading toolsets, exploring new technologies, and ensuring best-in-class engineering practices to deliver high-quality, data-led solutions for our customers.

Key Responsibilities

Design, build, and maintain reusable and scalable data pipelines at enterprise scale.

Work with structured and unstructured data to support machine learning models and deliver real-time contextual insights.

Evaluate and adopt new technologies to design and build scalable, real-time data applications.

Manage the full data lifecycle, working with modern and traditional data platforms (Kafka, GCP, SQL Server).

Collaborate with cross-functional teams to deliver robust data solutions and support data-driven decision making.

Adopt and promote best engineering practices including Test Driven Development (TDD), code reviews, CI/CD pipelines.

Mentor junior engineers and foster a culture of high-quality engineering and innovation.

Build strong working relationships and collaborate effectively across teams in diverse domains.

Must-Have Skills

Strong coding/scripting experience in Python, SQL, Java, Scala, or Go, with a solid foundation in commercial/industry projects.

Proven experience with operational data stores, data warehouses, big data technologies, and data lakes (e.g., BigQuery, Cloud Spanner).

Hands-on experience with relational and non-relational databases (SQL Server, Oracle, DB2) and strong knowledge of relational and dimensional data structures.

Solid experience with Kafka technologies for real-time data streaming.

Strong understanding of computer science fundamentals: data structures, algorithms, software design, design patterns, and core programming concepts.

Good understanding of cloud storage, networking, and resource provisioning within GCP.

Good-to-Have Skills

Certifications: GCP Professional Data Engineer, Apache Kafka (CCDAK).

Strong experience in data analysis and end-to-end data lifecycle management.

Knowledge of containers and orchestration (Docker, Kubernetes).

Familiarity with other cloud platforms (Azure, AWS).

Bonus (Preferred Experience)

Experience working in the banking and financial services domain.

Exposure to personalization and customer journey optimization using data-driven approaches.

Why Join Us?

Opportunity to work on cutting-edge GCP data engineering projects.

Collaborate with a team of passionate engineers delivering high-impact, customer-centric solutions.

Gain exposure to modern data platforms and advanced technologies.

Contribute to shaping personalized banking experiences powered by data.
