Job Description – GCP Data Engineer

Role Overview
We are seeking a highly skilled GCP Data Engineer to join our team and play a key role in building scalable, reusable, and high-performing data solutions. The ideal candidate will be responsible for designing and developing data pipelines, working with structured and unstructured data, enabling machine learning feature engineering, and delivering real-time contextual insights that power personalized customer journeys.

This role will involve working with industry-leading toolsets, exploring new technologies, and ensuring best-in-class engineering practices to deliver high-quality, data-led solutions for our customers.

Key Responsibilities

Design, build, and maintain reusable and scalable data pipelines at enterprise scale.

Work with structured and unstructured data to support machine learning models and deliver real-time contextual insights.

Evaluate and adopt new technologies to design and build scalable, real-time data applications.

Manage the full data lifecycle, working with modern and traditional data platforms (Kafka, GCP, SQL Server).

Collaborate with cross-functional teams to deliver robust data solutions and support data-driven decision making.

Adopt and promote best engineering practices including Test Driven Development (TDD), code reviews, CI/CD pipelines.

Mentor junior engineers and foster a culture of high-quality engineering and innovation.

Build strong working relationships and collaborate effectively across teams in diverse domains.

Must-Have Skills

Strong coding/scripting experience in Python, SQL, Java, Scala, or Go, with a solid foundation in commercial/industry projects.

Proven experience with operational data stores, data warehouses, big data technologies, and data lakes (e.g., BigQuery, Cloud Spanner).

Hands-on experience with relational and non-relational databases (SQL Server, Oracle, DB2) and strong knowledge of relational and dimensional data structures.

Solid experience with Kafka technologies for real-time data streaming.

Strong understanding of computer science fundamentals: data structures, algorithms, software design, design patterns, and core programming concepts.

Good understanding of cloud storage, networking, and resource provisioning within GCP.

Good-to-Have Skills

Certifications: GCP Professional Data Engineer, Apache Kafka (CCDAK).

Strong experience in data analysis and end-to-end data lifecycle management.

Knowledge of containers and orchestration (Docker, Kubernetes).

Familiarity with other cloud platforms (Azure, AWS).

Bonus (Preferred Experience)

Experience working in the banking and financial services domain.

Exposure to personalization and customer journey optimization using data-driven approaches.

Why Join Us?

Opportunity to work on cutting-edge GCP data engineering projects.










# *Job Description – GCP Data DevOps Engineer*

*Role Overview*
We are looking for a skilled *GCP Data DevOps Engineer* with strong expertise in both *data engineering* and *DevOps practices. The ideal candidate will design, build, and scale reusable data pipelines, manage the full data lifecycle, and bring modern DevOps principles to ensure automation, reliability, and scalability. You will work with structured and unstructured data, enable machine learning feature engineering, and deliver **real-time contextual insights* to power customer personalization journeys.

This is a hands-on engineering role requiring a blend of *data engineering, DevOps automation, and cloud-native expertise*.

---

### *Key Responsibilities*

* Design, develop, and maintain *scalable, reusable data pipelines* for structured/unstructured data.
* Enable *real-time data processing* and support feature engineering for ML models.
* Apply *DevOps practices* (CI/CD, automated testing, infrastructure as code, monitoring) to data engineering solutions.
* Work with *Kafka, GCP, SQL Server, and big data platforms* (BigQuery, Cloud Spanner) across the *entire data lifecycle*.
* Implement *automation for data pipelines*, cloud infrastructure, and deployment workflows.
* Collaborate with cross-functional teams to deliver *data-driven and customer-centric solutions*.
* Promote best practices like *Test Driven Development, code reviews, CI/CD pipelines*, and observability in data systems.
* Mentor engineers and foster knowledge sharing within the team.
* Build strong relationships across technology and business teams, working on *banking and personalization solutions*.

---

### *Must-Have Skills*

* Strong coding/scripting in *Python, SQL, Java, Scala, or Go*.
* Hands-on experience with *data warehouses, operational data stores, big data technologies, and data lakes* (e.g., BigQuery, Cloud Spanner).
* Experience with *relational & non-relational databases* (SQL Server, Oracle, DB2).
* *Kafka expertise* for streaming and real-time data processing.
* Strong *DevOps skills*:

  * CI/CD pipelines (Jenkins, GitLab CI, Cloud Build, or similar)
  * Infrastructure as Code (Terraform, Cloud Deployment Manager, Ansible)
  * Containerization & orchestration (*Docker, Kubernetes*)
  * Monitoring & logging (Prometheus, Grafana, ELK, GCP Stackdriver).
* Good understanding of *cloud storage, networking, and resource provisioning* in GCP.
* Strong computer science fundamentals (data structures, algorithms, design patterns).

---

### *Good-to-Have Skills*

* *Certifications*: GCP Professional Data Engineer, GCP DevOps Engineer, Apache Kafka (CCDAK).
* Experience in *data analysis* and *full data lifecycle management*.
* Familiarity with multi-cloud environments (AWS, Azure).

---

### *Bonus (Preferred Experience)*

* Prior experience in *banking and financial services*.
* Exposure to *personalization platforms and customer journey optimization*.

---

### *Why Join Us?*

* Be at the intersection of *Data Engineering and DevOps* – building modern, automated, and scalable data systems.
* Work on *cutting-edge GCP projects* with impact across banking and personalization domains.
* Collaborate with a talented, cross-functional team delivering *high-quality, customer-focused solutions*.

Collaborate with a team of passionate engineers delivering high-impact, customer-centric solutions.

Gain exposure to modern data platforms and advanced technologies.

Contribute to shaping personalized banking experiences powered by data.
